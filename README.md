# Ultralight RAG MCP - Second Brain Agent System

A lightweight, fully local "Second Brain" agent system combining RAG, multi-agent orchestration, MCP tools, and local LLM inference via Ollama.

## Features

- **Local RAG Pipeline** - FAISS vector store with sentence-transformer embeddings
- **Multi-Agent System** - Research, Execution, Memory, and Planner agents
- **MCP Integration** - Extensible tool protocol for filesystem and knowledge operations
- **Persistent Memory** - SQLite-backed conversation history and fact storage
- **Offline-First** - No external API dependencies at runtime

## Requirements

- **Python**: 3.10+ (3.12 recommended)
- **Ollama**: For local LLM inference
- **GPU**: 8GB VRAM recommended (RTX 2070 or equivalent)
- **RAM**: 8GB minimum, 16GB recommended

## Quick Start

### 1. Install Ollama

Download from [ollama.com](https://ollama.com) and install.

```bash
# Pull Ministral 3B - lightweight model for laptops
ollama pull ministral-3:3b
```

### 2. Set Up Python Environment

```powershell
# Create virtual environment
py -m venv venv

# Activate (Windows PowerShell)
.\venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt
```

### 3. Add Documents

Place your documents (markdown, text, PDF) in `data/documents/`.

### 4. Run

**Command Line Interface:**
```bash
py src/main.py
```

**Desktop GUI Application:**
```bash
py src/gui_app.py
```

### 5. Build Standalone .exe (Optional)

```bash
py build_exe.py
```

This creates `dist/SecondBrain.exe` - a standalone executable that can run without Python installed.

## Desktop GUI Features

The GUI provides a modern dark-themed interface with:

- **ðŸ¤– Model Selector** - Switch between Ollama models on the fly
- **ðŸ“š File Manager** - Upload and manage documents (drag & drop supported)
- **ðŸ’¬ Chat Panel** - Streaming responses with conversation history
- **âš™ï¸ Settings** - Adjust temperature, token limits, and retrieval settings

> **Tip for Laptops**: Use smaller models like `phi:2.7b`, `gemma:2b`, or `tinyllama:1.1b` for faster responses without a GPU.

## Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py              # Entry point & CLI
â”‚   â”œâ”€â”€ agents/              # Multi-agent system
â”‚   â”‚   â”œâ”€â”€ orchestrator.py  # Query routing & coordination
â”‚   â”‚   â”œâ”€â”€ specialized.py   # Research, Execution, Memory, Planner agents
â”‚   â”‚   â”œâ”€â”€ memory.py        # Conversation & fact memory
â”‚   â”‚   â””â”€â”€ personas.py      # Agent role definitions
â”‚   â”œâ”€â”€ rag/                 # RAG components
â”‚   â”‚   â”œâ”€â”€ ingestion.py     # Document loading & chunking
â”‚   â”‚   â”œâ”€â”€ retrieval.py     # Query retrieval pipeline
â”‚   â”‚   â”œâ”€â”€ vectorstore.py   # FAISS vector store manager
â”‚   â”‚   â””â”€â”€ embeddings.py    # Embedding model wrapper
â”‚   â”œâ”€â”€ mcp/                 # MCP tool registry
â”‚   â””â”€â”€ utils/               # Configuration & helpers
â”œâ”€â”€ mcp_servers/             # MCP tool servers
â”‚   â”œâ”€â”€ filesystem.py        # File I/O operations
â”‚   â””â”€â”€ knowledge_api.py     # Knowledge base operations
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/           # Source documents
â”‚   â””â”€â”€ vectorstore/         # FAISS index (auto-generated)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.json          # Configuration
â””â”€â”€ docs/
    â””â”€â”€ performance_analysis.md
```

## Configuration

Edit `config/config.json` to customize:

```json
{
  "llm": {
    "model": "mistral",
    "temperature": 0.3
  },
  "rag": {
    "chunk_size": 512,
    "retrieval_k": 3
  },
  "memory": {
    "persistence": true,
    "db_path": "./data/memory.db"
  }
}
```

## Usage

The system provides an interactive CLI with the following commands:

| Command | Description |
|---------|-------------|
| Type a question | Query the knowledge base |
| `/ingest` | Ingest documents from data/documents/ |
| `/memory` | View memory statistics |
| `/agents` | List available agents |
| `/help` | Show available commands |
| `/quit` | Exit the application |

## Performance

| Metric | Expected |
|--------|----------|
| Query latency | 3-6 seconds |
| Embedding generation | 50-200ms |
| Token generation | 10-12 tok/s |
| VRAM usage | 4-6GB |

## Project Status

- **Phase 1**: Core Setup (RAG + LLM) âœ…
- **Phase 2**: Multi-Agent System âœ…
- **Phase 3**: MCP Integration âœ…
- **Phase 4**: Knowledge Base & Persistence âœ…
- **Phase 5**: Testing & Optimization âœ…

## License

MIT
